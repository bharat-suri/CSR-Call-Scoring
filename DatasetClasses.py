# -*- coding: utf-8 -*-
"""DatasetClasses

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WGuilwMSn2xsfnKuw4yy27BkIlL_8A-h
"""

from torch.utils.data import Dataset
import numpy as np
import pandas as pd
import json
import torch
from collections import Counter
from torchtext.vocab import Vocab
from torchtext.data.utils import get_tokenizer
import nltk
from nltk.tokenize import sent_tokenize
from Preprocessing import preprocess_transcript

class YelpDataset(Dataset):
    """Yelp dataset."""

    def __init__(self, file_name):
        """
        Args:
            file_name: The json file to make the dataset from
        """
        self.df = pd.read_json(file_name, lines=True)
        word_tokenizer = get_tokenizer('basic_english')
        
        binary_cat = []
        counter = Counter()
        reviews = []

        #Create target class for each review, build vocab
        for index, row in self.df.iterrows():
            binary_cat.append(row['category'])

            sentences = sent_tokenize(row['text'])
            sentences = [s.replace('.', '') for s in sentences]
            reviews.append(sentences)
            for i in range(len(sentences)):
              words = word_tokenizer(sentences[i])
              counter.update(words)

        self.vocab = Vocab(counter, min_freq=1)
        self.df['category'] = binary_cat
        self.df['text'] = reviews
        


    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        category = self.df.iloc[idx, 0]
        text = self.df.iloc[idx, 1]
        sample = {'category': category, 'text': text}

        return sample

    def get_vocab(self):
      return self.vocab

class CallDataset(Dataset):
    """Call transcript dataset."""

    def __init__(self, files, classifications):
        """
        Args:
            file_name: The json file to make the dataset from
        """
        self.df = pd.DataFrame()
        word_tokenizer = get_tokenizer('basic_english')
        
        clean_files = []
        for f in files:
          clean_files.append(preprocess_transcript(f))

        counter = Counter()

        #Build vocab from transcripts
        for transcript in clean_files:
          for i in range(len(transcript)):
            words = word_tokenizer(transcript[i])
            counter.update(words)

        self.vocab = Vocab(counter, min_freq=1)
        self.df['category'] = classifications
        self.df['text'] = clean_files
        


    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        category = self.df.iloc[idx, 0]
        text = self.df.iloc[idx, 1]
        sample = {'category': category, 'text': text}

        return sample

    def get_vocab(self):
        return self.vocab